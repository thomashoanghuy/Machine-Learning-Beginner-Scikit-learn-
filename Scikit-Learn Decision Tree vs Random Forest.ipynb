{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58e42759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13b47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_file_path = \"C:/Users/user/Desktop/School/Data Projects/Melbourne Housing Snapshot/melb_data.csv\"\n",
    "melbourne_data = pd.read_csv(melbourne_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7f2ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Rooms         Price      Distance      Postcode      Bedroom2  \\\n",
      "count  13580.000000  1.358000e+04  13580.000000  13580.000000  13580.000000   \n",
      "mean       2.937997  1.075684e+06     10.137776   3105.301915      2.914728   \n",
      "std        0.955748  6.393107e+05      5.868725     90.676964      0.965921   \n",
      "min        1.000000  8.500000e+04      0.000000   3000.000000      0.000000   \n",
      "25%        2.000000  6.500000e+05      6.100000   3044.000000      2.000000   \n",
      "50%        3.000000  9.030000e+05      9.200000   3084.000000      3.000000   \n",
      "75%        3.000000  1.330000e+06     13.000000   3148.000000      3.000000   \n",
      "max       10.000000  9.000000e+06     48.100000   3977.000000     20.000000   \n",
      "\n",
      "           Bathroom           Car       Landsize  BuildingArea    YearBuilt  \\\n",
      "count  13580.000000  13518.000000   13580.000000   7130.000000  8205.000000   \n",
      "mean       1.534242      1.610075     558.416127    151.967650  1964.684217   \n",
      "std        0.691712      0.962634    3990.669241    541.014538    37.273762   \n",
      "min        0.000000      0.000000       0.000000      0.000000  1196.000000   \n",
      "25%        1.000000      1.000000     177.000000     93.000000  1940.000000   \n",
      "50%        1.000000      2.000000     440.000000    126.000000  1970.000000   \n",
      "75%        2.000000      2.000000     651.000000    174.000000  1999.000000   \n",
      "max        8.000000     10.000000  433014.000000  44515.000000  2018.000000   \n",
      "\n",
      "          Lattitude    Longtitude  Propertycount  \n",
      "count  13580.000000  13580.000000   13580.000000  \n",
      "mean     -37.809203    144.995216    7454.417378  \n",
      "std        0.079260      0.103916    4378.581772  \n",
      "min      -38.182550    144.431810     249.000000  \n",
      "25%      -37.856822    144.929600    4380.000000  \n",
      "50%      -37.802355    145.000100    6555.000000  \n",
      "75%      -37.756400    145.058305   10331.000000  \n",
      "max      -37.408530    145.526350   21650.000000  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4021f8e",
   "metadata": {},
   "source": [
    "# Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40ceb72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Rooms         Price     Distance     Postcode     Bedroom2  \\\n",
      "count  6196.000000  6.196000e+03  6196.000000  6196.000000  6196.000000   \n",
      "mean      2.931407  1.068828e+06     9.751097  3101.947708     2.902034   \n",
      "std       0.971079  6.751564e+05     5.612065    86.421604     0.970055   \n",
      "min       1.000000  1.310000e+05     0.000000  3000.000000     0.000000   \n",
      "25%       2.000000  6.200000e+05     5.900000  3044.000000     2.000000   \n",
      "50%       3.000000  8.800000e+05     9.000000  3081.000000     3.000000   \n",
      "75%       4.000000  1.325000e+06    12.400000  3147.000000     3.000000   \n",
      "max       8.000000  9.000000e+06    47.400000  3977.000000     9.000000   \n",
      "\n",
      "          Bathroom          Car      Landsize  BuildingArea    YearBuilt  \\\n",
      "count  6196.000000  6196.000000   6196.000000   6196.000000  6196.000000   \n",
      "mean      1.576340     1.573596    471.006940    141.568645  1964.081988   \n",
      "std       0.711362     0.929947    897.449881     90.834824    38.105673   \n",
      "min       1.000000     0.000000      0.000000      0.000000  1196.000000   \n",
      "25%       1.000000     1.000000    152.000000     91.000000  1940.000000   \n",
      "50%       1.000000     1.000000    373.000000    124.000000  1970.000000   \n",
      "75%       2.000000     2.000000    628.000000    170.000000  2000.000000   \n",
      "max       8.000000    10.000000  37000.000000   3112.000000  2018.000000   \n",
      "\n",
      "         Lattitude   Longtitude  Propertycount  \n",
      "count  6196.000000  6196.000000    6196.000000  \n",
      "mean    -37.807904   144.990201    7435.489509  \n",
      "std       0.075850     0.099165    4337.698917  \n",
      "min     -38.164920   144.542370     389.000000  \n",
      "25%     -37.855438   144.926198    4383.750000  \n",
      "50%     -37.802250   144.995800    6567.000000  \n",
      "75%     -37.758200   145.052700   10175.000000  \n",
      "max     -37.457090   145.526350   21650.000000  \n",
      "Average Landsize is  471.00693996126535\n",
      "Newest Unit's age is  5.0\n"
     ]
    }
   ],
   "source": [
    "table_summary = melbourne_data.describe()\n",
    "print(table_summary)\n",
    "\n",
    "#Average Landsize for Melbourne properties\n",
    "avg_lot_size = table_summary['Landsize']['mean']\n",
    "print(\"Average Landsize is \" , avg_lot_size)\n",
    "\n",
    "newest_home_age = 2023 - table_summary['YearBuilt']['max']\n",
    "print(\"Newest Unit's age is \" , newest_home_age)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd8efdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check if there is any missing data in our dataset.\n",
    "melbourne_data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3f02a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are missing data (na) in our dataset, so it's better if we just drop these NA data for now.\n",
    "melbourne_data = melbourne_data.dropna(axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe8e2365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        1035000.0\n",
      "2        1465000.0\n",
      "4        1600000.0\n",
      "6        1876000.0\n",
      "7        1636000.0\n",
      "           ...    \n",
      "12205     601000.0\n",
      "12206    1050000.0\n",
      "12207     385000.0\n",
      "12209     560000.0\n",
      "12212    2450000.0\n",
      "Name: Price, Length: 6196, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Select our target variable (the variable which we want to predict)\n",
    "# In this case, it is Price of the property\n",
    "\n",
    "y = melbourne_data.Price\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e35c8d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG',\n",
       "       'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',\n",
       "       'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',\n",
       "       'Longtitude', 'Regionname', 'Propertycount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next let's see all the columns of these dataset that we are working with.\n",
    "melbourne_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7afc5632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Rooms  Bathroom  Landsize  Lattitude  Longtitude\n",
      "1          2       1.0     156.0  -37.80790   144.99340\n",
      "2          3       2.0     134.0  -37.80930   144.99440\n",
      "4          4       1.0     120.0  -37.80720   144.99410\n",
      "6          3       2.0     245.0  -37.80240   144.99930\n",
      "7          2       1.0     256.0  -37.80600   144.99540\n",
      "...      ...       ...       ...        ...         ...\n",
      "12205      3       2.0     972.0  -37.51232   145.13282\n",
      "12206      3       1.0     179.0  -37.86558   144.90474\n",
      "12207      1       1.0       0.0  -37.85588   144.89936\n",
      "12209      2       1.0       0.0  -37.85581   144.99025\n",
      "12212      6       3.0    1087.0  -37.81038   144.89389\n",
      "\n",
      "[6196 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Now I don't know much about housing prices, but to predict our target variable, we must select some columns from these columns.\n",
    "#Let's just select the obvious predictors that will definitely affect housing prices.\n",
    "\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\n",
    "#we call all these predictors X\n",
    "X = melbourne_data[melbourne_features]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62fc8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Great, now essentially machine learning model is mathematical functions that calculate y from x.\n",
    "#for example, Linear Regression, the easiest to understand, y = m * X + c (m being the gradient of the graph, c is the constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad0515",
   "metadata": {},
   "source": [
    "# Let's build simply model using Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27d1083b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split into validation and training data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf9c8f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(random_state=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melbourne_model = DecisionTreeRegressor(random_state=1)\n",
    "#we set random state = 1, so that our model has the same result each runs.\n",
    "\n",
    "#Fit Model\n",
    "melbourne_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bf824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#That is it. A simple model of Decision Tree regressor (readme. for explanation on Decision Tree logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a68a4d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for the following 5 houses:\n",
      "      Rooms  Bathroom  Landsize  Lattitude  Longtitude\n",
      "6048      3       3.0     221.0  -37.77080    144.8401\n",
      "9186      4       2.0     528.0  -37.83539    145.0431\n",
      "3991      3       2.0       0.0  -37.80950    144.9691\n",
      "5829      3       2.0    1039.0  -37.86380    144.9820\n",
      "3616      6       6.0    1334.0  -37.80290    145.0267\n"
     ]
    }
   ],
   "source": [
    "#let's test our model\n",
    "print(\"Making predictions for the following 5 houses:\")\n",
    "test1 = val_X.head(5)\n",
    "print(test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0ed9dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predictions are\n",
      "[ 503000. 1857000.  760000. 1395000. 4250000.]\n"
     ]
    }
   ],
   "source": [
    "print(\"The predictions are\")\n",
    "prediction1 = melbourne_model.predict(test1)\n",
    "print(prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90caaf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6048     620000.0\n",
      "9186    2320000.0\n",
      "3991     750000.0\n",
      "5829    1120000.0\n",
      "3616    6500000.0\n",
      "Name: Price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Let's compare with the actual prices of these units in our original dataset ()\n",
    "print(val_y.head(5))\n",
    "val_test = val_y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5637261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As you can see, using validation data set x and validation predictor y (val_y) , we realize that the prediction is not exactly accurate\n",
    "#Let's see if we can optimize these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb27ba22",
   "metadata": {},
   "source": [
    "# Model Optimization part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d55793fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree model is essentially keep asking Yes/No question to split the dataset into smaller leaf, so that maybe the nth level of the segmented leaf, will have prices of houses with similar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4eec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example: First level split can be \"Do the house has more than 2 bedroom??\"\n",
    "#Next level can be: \"Lotsize larger than 8500 squarefeet?\" for the houses with less than 2 bedroom\n",
    "#\"Lotsize larger than 11500 squarefeet?\" for house with more than 2 bedrooms.\n",
    "#Keep splitting the data using segmentation of the housing under 5 predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90cb33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overfitting: when you segment dataset into too small leaf. Means 1 leaf contains only small number of houses.\n",
    "#If we insert a brandnew house, it can only fit into 1 single leaf (that satisfy all the split conditions), \n",
    "# Predicted price will may not be accurate, because it's made with only small number of data inside that 1 small leaf.\n",
    "\n",
    "#Underfitting: is opposite with the above issue. Where the split is too shallow, one single leaf contains far too much data of houses\n",
    "#This leads to a problem where if you fit in a brand new house, it can easily fit into any of the big leaf and result in inaccurate prediction as well\n",
    "\n",
    "#The ideal scenerio is to find out the Mean Absolute Error ( that sweet spot where we can reduce the lowest level of underfitting and overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ff1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use another tool under SkLearn, that is mean_absolute_error\n",
    "#the better Decision Tree model, will have lower MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d1dea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error\n",
      "623000.0\n"
     ]
    }
   ],
   "source": [
    "val_mae = mean_absolute_error(val_test ,prediction1)\n",
    "print(\"Mean Absolute Error\")\n",
    "print(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d78e43",
   "metadata": {},
   "source": [
    "# Model Optimization part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaab7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the above model is only has ONE MAE, which we cannot compare to any other models. \n",
    "#That's why when we build ML model as beginner, it's best that we should write them as function-based coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a23073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our data are the same: train_X, val_X, train_y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "649876c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(max_leaf_nodes , train_x , val_x , train_y , val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_x , train_y)\n",
    "    prediction2 = model.predict(val_x)\n",
    "    mae = mean_absolute_error(val_y , prediction2)\n",
    "    return (mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "12c30a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can apply this function above to multiple numbers of nodes (splits)\n",
    "#remember that a leaf is a node without children.\n",
    "\n",
    "# Binary tree Theorem:\n",
    "# no. of internal node = i\n",
    "# number of nodes (both internal nodes and leaves) : n =2i + 1\n",
    "# (or vice versa : number of internal node = (n-1) / 2 )\n",
    "# number of leaves: l = (n+1)/ 2\n",
    "# vice versa: number of  nodes from leaves :  n = 2l -1 \n",
    "\n",
    "# hence relationship between internal nodes vs leaves \n",
    "# 2i + 1 = 2l -1 => i = l -1 \n",
    "\n",
    "# maximum number of leaves = max-l = 2^(h-1) where h is the height of the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8994c013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max leaf node:  5 MAE is  369673.0400167675\n",
      "For max leaf node:  50 MAE is  266644.21831092256\n",
      "For max leaf node:  500 MAE is  243613.31456921576\n",
      "For max leaf node:  5000 MAE is  256227.639767592\n"
     ]
    }
   ],
   "source": [
    "for max_leaf_nodes in [5,50,500,5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes , train_X , val_X , train_y, val_y)\n",
    "    print(\"For max leaf node: \" , max_leaf_nodes , \"MAE is \" , my_mae )\n",
    "    \n",
    "\n",
    "#As you can see, for max leaf node of 500, the MAE is the lowest, hence the optimal MAE would be somewhere between 50 - 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643ba9d",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc529e",
   "metadata": {},
   "source": [
    "### A decision tree is a simple, yet powerful algorithm that creates a tree-like model of decisions and their possible consequences. The algorithm splits the data into smaller subsets based on the features that are most informative for predicting the outcome. Each decision is made based on the values of one feature, and the outcome is determined by the final leaf node of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8059f6a2",
   "metadata": {},
   "source": [
    "### Random Forest is an extension of decision trees that improves the accuracy and robustness of the algorithm by using an ensemble approach. Random Forest creates multiple decision trees, each with a different subset of the training data and features. During the prediction phase, the algorithm combines the predictions of all the decision trees to produce a final prediction. By creating multiple trees, Random Forest reduces the risk of overfitting and increases the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c12568b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically, with Decision tree, there are so only so many height of 1 single tree can go, before it reaches the level of overfitting\n",
    "# Hence if we build multiple decision trees and combine their results, we can reduce the risks of overfitting.\n",
    "#let'S build Random Forest model with the above sets of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f808bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae2(max_leaf_nodes , train_x , val_x , train_y , val_y):\n",
    "    forest_model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes , random_state=1)\n",
    "    forest_model.fit(train_x , train_y)\n",
    "    prediction3 = forest_model.predict(val_x)\n",
    "    mae = mean_absolute_error(val_y , prediction3)\n",
    "    return (mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4e044a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369673.0400167675, 266644.21831092256, 243613.31456921576, 256227.639767592]\n",
      "[352296.73817633535, 232778.04369765415, 193927.13612872423, 190556.59504186007]\n"
     ]
    }
   ],
   "source": [
    "leaf_nodes = [5,50,500,5000]\n",
    "list1 = []\n",
    "list2 = []\n",
    "for max_leaf_nodes in leaf_nodes:\n",
    "    decisiontree_mae = get_mae(max_leaf_nodes , train_X , val_X , train_y, val_y)\n",
    "    list1.append(decisiontree_mae)\n",
    "    randomforest_mae = get_mae2(max_leaf_nodes , train_X , val_X , train_y, val_y)\n",
    "    list2.append(randomforest_mae)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a9edd04e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         5              50             500            5000\n",
      "Decision Trees  369673.040017  266644.218311  243613.314569  256227.639768\n",
      " Random Forest  352296.738176  232778.043698  193927.136129  190556.595042\n"
     ]
    }
   ],
   "source": [
    "list3 = [list1 , list2]\n",
    "rows_name = [\"Decision Trees\" ,\" Random Forest\"]\n",
    "comparison_table = pd.DataFrame(list3, columns = leaf_nodes , index =rows_name)\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae89339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on MAE, the Random Forest has reduced MAE significantly compared to Decision Trees.\n",
    "#Even at 5000 nodes, Random Forest MAE is still decreasing, means the optimal MAE point for Random Forest model on this data is still further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a582f",
   "metadata": {},
   "source": [
    "### Remember that, the above models are only using FIVE predictors out multiple predictors in the original dataset. Of course, the more predictors should, in theory, increase the accuracy of the models. But that is not always the case. \n",
    "### We need to do more in the step EDA to explore the predictors and transform / eliminate predictors that are subjected other issues such as imbalance data, highly skewed , outliers etc..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
